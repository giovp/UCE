{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4018ee",
   "metadata": {},
   "source": [
    "# Embedding Novel Species\n",
    "\n",
    "This notebook will create the files you need to embed a novel species that wasn't included in the training data.\n",
    "\n",
    "To start, you will need to download the ESM2 protein embeddings and the reference proteome for the species.\n",
    "\n",
    "You can find precalculated ESM2 protein embeddings for many species [here](https://drive.google.com/drive/folders/1_Dz7HS5N3GoOAG6MdhsXWY1nwLoN13DJ?usp=drive_link)\n",
    "\n",
    "For reference proteomes, you can download them from [here](https://useast.ensembl.org/info/about/species.html).\n",
    "\n",
    "If there is no protein embedding for the species you are interested in, you can request to have it made via Github or email, or you can create it yourself following instructions [here](https://github.com/snap-stanford/SATURN/tree/main/protein_embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab368d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b189335",
   "metadata": {},
   "source": [
    "## Convert ESM2 protien embeddings to UCE format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75fa0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_hdf5(file_path):\n",
    "    \"\"\"Load dictionary from HDF5 file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to HDF5 file containing embeddings data. The file should have\n",
    "            a 'keys' dataset containing gene names and an 'arrays' group containing the\n",
    "            corresponding embedding arrays.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping gene names (str) to their embedding arrays (numpy.ndarray).\n",
    "            The keys are decoded from bytes to UTF-8 strings.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        # Get the keys\n",
    "        keys = [k.decode(\"utf-8\") for k in f[\"keys\"][:]]\n",
    "\n",
    "        # Load the arrays\n",
    "        arrays_group = f[\"arrays\"]\n",
    "        for key in keys:\n",
    "            data_dict[key] = arrays_group[str(key)][:]\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def filter_out_ensembl(PE):\n",
    "    total_before = len(PE.keys())\n",
    "\n",
    "    # Count keys that start with \"ENS\"\n",
    "    ens_count = sum(1 for key in PE.keys() if key.startswith(\"ENS\"))\n",
    "\n",
    "    # Calculate remaining after filtering\n",
    "    remaining = total_before - ens_count\n",
    "\n",
    "    print(f\"Total keys before filtering: {total_before}\")\n",
    "    print(f\"Keys starting with 'ENS' to be filtered: {ens_count}\")\n",
    "    print(f\"Keys remaining after filtering: {remaining}\")\n",
    "\n",
    "    # Remove all key-value pairs that start with \"ENS\"\n",
    "    PE = {k: v for k, v in PE.items() if not k.startswith(\"ENS\")}\n",
    "\n",
    "    return PE\n",
    "\n",
    "def convert_hdf5_to_pt(hdf5_path, output_path):\n",
    "    emb = load_from_hdf5(hdf5_path)\n",
    "    emb_tensors = {k: torch.from_numpy(v) for k, v in emb.items()}\n",
    "\n",
    "    # Filter out ENSP IDs\n",
    "    emb_tensors = filter_out_ensembl(emb_tensors)\n",
    "\n",
    "    # check dim of first tensor\n",
    "    print(emb_tensors[list(emb_tensors.keys())[0]].shape)\n",
    "    torch.save(emb_tensors, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9a306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIES_NAME = \"platypus\" # short hand name for this species, will be used in arguments and files\n",
    "H5_FILE_PATH = \"/mnt/czi-sci-ai/generate-cross-species-secondary/protein_embedding_data/pkl/ornithorhynchus_anatinus_gene_large.h5\"\n",
    "\n",
    "OUTPUT_DIR = \"/mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280\"\n",
    "\n",
    "MODEL_FILES_PATH = \"/mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/model_files\"\n",
    "PROTEIN_EMBEDDINGS_PATH = f\"{MODEL_FILES_PATH}/protein_embeddings\"\n",
    "NEW_SPECIES_CSV_PATH = f\"{MODEL_FILES_PATH}/new_species_protein_embeddings.csv\"\n",
    "\n",
    "# Path to the species proteome\n",
    "SPECIES_PROTEIN_FASTA_PATH = f\"/mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/fasta/Ornithorhynchus_anatinus.mOrnAna1.p.v1.pep.all.fa\"\n",
    "\n",
    "# Path to the ESM2 Embeddings\n",
    "SPECIES_PROTEIN_EMBEDDINGS_PATH = f\"{PROTEIN_EMBEDDINGS_PATH}/Ornithorhynchus_anatinus_ESM2.pt\"\n",
    "\n",
    "# primary_assembly name, this needs to be matched to the FASTA file\n",
    "ASSEMBLY_NAME = \"mOrnAna1.p.v1\"\n",
    "# NCBI Taxonomy ID, please set this so that if someone else also embeds the same species,\n",
    "# randomly generated chromosome tokens will be the same\n",
    "TAXONOMY_ID = 9258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfaec308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keys before filtering: 17418\n",
      "Keys starting with 'ENS' to be filtered: 6089\n",
      "Keys remaining after filtering: 11329\n",
      "torch.Size([5120])\n"
     ]
    }
   ],
   "source": [
    "convert_hdf5_to_pt(H5_FILE_PATH, SPECIES_PROTEIN_EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d37e52",
   "metadata": {},
   "source": [
    "You can view the FASTA format here, please confirm the primary_assembly name is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ecf1464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">ENSOANP00000024997.1 pep primary_assembly:mOrnAna1.p.v1:MT:2807:3763:1 gene:ENSOANG00000019388.1 transcript:ENSOANT00000028512.1 gene_biotype:protein_coding transcript_biotype:protein_coding gene_symbol:ND1 description:NADH dehydrogenase subunit 1 [Source:NCBI gene (formerly Entrezgene);Acc:808708]\n",
      "MFLVNLLILIIPVLLAVAFLTLLERKILGYMQFRKGPNIVGAHGLLQPIADAVKLFTKEP\n",
      "LRPLTSSIYMFILAPILALSLALTIWIPLPMPLPLIDLNLGLLFVLSVSGLSVYSILWSG\n",
      "WASNSKYALTGALRAVAQTISYEVTLAIILLSIMLINGSFTLTTLNLTQEYMWLIVPTWP\n",
      "LMLMWFISTLAETNRAPFDLTEGESELVSGFNVEYAAGPFAMFFLAEYANIIIMNALTVI\n",
      "LFFGTYHLIFLPEMSTTTFMIKTMLLTSLFLWIRASYPRFRYDQLMHLLWKNFLPITLVT\n",
      "CLWYIMLPTTLSGLPPQM\n",
      ">ENSOANP00000024996.2 pep primary_assembly:mOrnAna1.p.v1:MT:3971:5014:1 gene:ENSOANG00000019384.2 transcript:ENSOANT00000028508.2 gene_biotype:protein_coding transcript_biotype:protein_coding gene_symbol:ND2 description:NADH dehydrogenase subunit 2 [Source:NCBI gene (formerly Entrezgene);Acc:808700]\n",
      "MTPMTTLIMLFSLLLGTTLTLTSSHWLLMWMGLEVSTLAIIPLLTYTNHPRSIESAIKYF\n",
      "LTQATASMLLMFAASLNTWMTGHWTLMQIDNTISSGIMTFALAMKLGLAPFHYWVPEVLQ\n"
     ]
    }
   ],
   "source": [
    "!head {SPECIES_PROTEIN_FASTA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90540d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_to_paths = {\n",
    "    SPECIES_NAME: SPECIES_PROTEIN_FASTA_PATH,\n",
    "}\n",
    "\n",
    "species_to_ids = {\n",
    "    SPECIES_NAME: ASSEMBLY_NAME,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "623b99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_def = []\n",
    "\n",
    "missing_genes = {}\n",
    "for species in species_to_ids.keys():\n",
    "    missing_genes[species] = []\n",
    "    proteome_path = species_to_paths[species]\n",
    "    species_id = species_to_ids[species]\n",
    "\n",
    "    with open(proteome_path) as f:\n",
    "        proteome_lines = f.readlines()\n",
    "\n",
    "    gene_symbol_to_location = {}\n",
    "    gene_symbol_to_chrom = {}\n",
    "\n",
    "    for line in proteome_lines:\n",
    "        if line.startswith(\">\"):\n",
    "            split_line = line.split()\n",
    "            gene_symbol = [token for token in split_line if token.startswith(\"gene_symbol\")]\n",
    "            if len(gene_symbol) > 0:\n",
    "                gene_symbol = gene_symbol[0].split(\":\")\n",
    "                \n",
    "                if len(gene_symbol) == 2:\n",
    "                    gene_symbol = gene_symbol[1]\n",
    "                elif len(gene_symbol) > 2:\n",
    "                    gene_symbol = \":\".join(gene_symbol[1:]) # fix for annoying zebrafish gene names with colons in them\n",
    "                else:\n",
    "                    1/0 # something weird happening, throw an error\n",
    "                \n",
    "                \n",
    "                chrom = None\n",
    "                \n",
    "                chrom_arr = [token for token in split_line if token.startswith(\"chromosome:\")]\n",
    "                if len(chrom_arr) > 0:\n",
    "                    chrom = chrom_arr[0].replace(\"chromosome:\", \"\")\n",
    "                else:\n",
    "                    chrom_arr = [token for token in split_line if token.startswith(\"primary_assembly:\")]\n",
    "                    if len(chrom_arr) > 0:\n",
    "                        chrom = chrom_arr[0].replace(\"primary_assembly:\", \"\")\n",
    "                    else:\n",
    "                        chrom_arr = [token for token in split_line if token.startswith(\"scaffold:\")] \n",
    "                        if len(chrom_arr) > 0:\n",
    "                            chrom = chrom_arr[0].replace(\"scaffold:\", \"\")\n",
    "                if chrom is not None:\n",
    "                    gene_symbol_to_location[gene_symbol] = chrom.split(\":\")[2]\n",
    "                    gene_symbol_to_chrom[gene_symbol] = chrom.split(\":\")[1]\n",
    "                else:\n",
    "                    missing_genes[species].append(gene_symbol)\n",
    "                    \n",
    "\n",
    "    positional_df = pd.DataFrame()\n",
    "    positional_df[\"gene_symbol\"] = [gn.upper() for gn in list(gene_symbol_to_chrom.keys())]\n",
    "    positional_df[\"chromosome\"] = list(gene_symbol_to_chrom.values())\n",
    "    positional_df[\"start\"] = list(gene_symbol_to_location.values())\n",
    "    positional_df = positional_df.sort_values([\"chromosome\", \"start\"])\n",
    "    #positional_df = positional_df.set_index(\"gene_symbol\")\n",
    "    positional_df[\"species\"] = species\n",
    "    all_pos_def.append(positional_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b72887b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>start</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>MTLN</td>\n",
       "      <td>1</td>\n",
       "      <td>100009143</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>SLC19A3</td>\n",
       "      <td>1</td>\n",
       "      <td>100065503</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>DAW1</td>\n",
       "      <td>1</td>\n",
       "      <td>100217380</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>EIF2A</td>\n",
       "      <td>1</td>\n",
       "      <td>100455816</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>TSC22D2</td>\n",
       "      <td>1</td>\n",
       "      <td>100555135</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>ORNANAV1R3129</td>\n",
       "      <td>X5</td>\n",
       "      <td>9703484</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>CHRNB2</td>\n",
       "      <td>X5</td>\n",
       "      <td>971487</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>UBE2Q1</td>\n",
       "      <td>X5</td>\n",
       "      <td>983164</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4620</th>\n",
       "      <td>ORNANAV1R3193</td>\n",
       "      <td>X5</td>\n",
       "      <td>9924327</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8386</th>\n",
       "      <td>SHE</td>\n",
       "      <td>X5</td>\n",
       "      <td>997317</td>\n",
       "      <td>platypus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11330 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gene_symbol chromosome      start   species\n",
       "62             MTLN          1  100009143  platypus\n",
       "82          SLC19A3          1  100065503  platypus\n",
       "117            DAW1          1  100217380  platypus\n",
       "135           EIF2A          1  100455816  platypus\n",
       "152         TSC22D2          1  100555135  platypus\n",
       "...             ...        ...        ...       ...\n",
       "4571  ORNANAV1R3129         X5    9703484  platypus\n",
       "8304         CHRNB2         X5     971487  platypus\n",
       "8351         UBE2Q1         X5     983164  platypus\n",
       "4620  ORNANAV1R3193         X5    9924327  platypus\n",
       "8386            SHE         X5     997317  platypus\n",
       "\n",
       "[11330 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_pos_def = pd.concat(all_pos_def)\n",
    "master_pos_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d9dac28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "platypus    11330\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_pos_def[\"species\"].value_counts() # double check how many genes are mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a3d45c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platypus: 0\n"
     ]
    }
   ],
   "source": [
    "for k, v in missing_genes.items():\n",
    "    print(f\"{k}: {len(v)}\") # are any genes missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c59774b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "platypus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chromosome\n",
       "2                 808\n",
       "3                 761\n",
       "1                 732\n",
       "5                 717\n",
       "X1                660\n",
       "4                 595\n",
       "11                591\n",
       "7                 501\n",
       "10                499\n",
       "X5                486\n",
       "16                473\n",
       "14                421\n",
       "8                 416\n",
       "13                371\n",
       "9                 363\n",
       "17                353\n",
       "X2                333\n",
       "12                277\n",
       "19                275\n",
       "6                 267\n",
       "18                261\n",
       "21                251\n",
       "20                245\n",
       "15                210\n",
       "X3                158\n",
       "RZJT01000103.1    117\n",
       "X4                106\n",
       "RZJT01000302.1     16\n",
       "MT                 13\n",
       "RZJT01000086.1      8\n",
       "RZJT01000295.1      7\n",
       "RZJT01000087.1      6\n",
       "RZJT01000091.1      4\n",
       "RZJT01000067.1      4\n",
       "RZJT01000185.1      4\n",
       "RZJT01000140.1      4\n",
       "RZJT01000231.1      3\n",
       "RZJT01000030.1      2\n",
       "RZJT01000029.1      1\n",
       "RZJT01000046.1      1\n",
       "RZJT01000084.1      1\n",
       "RZJT01000072.1      1\n",
       "RZJT01000125.1      1\n",
       "RZJT01000090.1      1\n",
       "RZJT01000256.1      1\n",
       "RZJT01000180.1      1\n",
       "RZJT01000141.1      1\n",
       "RZJT01000194.1      1\n",
       "RZJT01000174.1      1\n",
       "RZJT01000300.1      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n"
     ]
    }
   ],
   "source": [
    "# Count genes per chromosome\n",
    "for species in species_to_ids.keys():\n",
    "    print(\"*********\")\n",
    "    print(species)\n",
    "    display(master_pos_def[master_pos_def[\"species\"] == species][\"chromosome\"].value_counts().head(50))\n",
    "    print(\"*********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d4001",
   "metadata": {},
   "source": [
    "## Filter ESM2 embeddings to only include genes in the master_pos_def\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8331053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10509/1721332365.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  PE = torch.load(SPECIES_PROTEIN_EMBEDDINGS_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes in PE before filtering: 11329\n",
      "Number of genes in PE after filtering: 11329\n",
      "Number of genes in master_pos_def after filtering: 11329\n"
     ]
    }
   ],
   "source": [
    "PE = torch.load(SPECIES_PROTEIN_EMBEDDINGS_PATH)\n",
    "print(f\"Number of genes in PE before filtering: {len(PE)}\")\n",
    "# Find intersection of gene symbols between PE and master_pos_def\n",
    "pe_genes = {k.upper() for k in PE.keys()}\n",
    "master_genes = set(master_pos_def[\"gene_symbol\"].unique())\n",
    "common_genes = pe_genes & master_genes\n",
    "# Filter both PE and master_pos_def to only include common genes\n",
    "PE = {k.upper(): v for k, v in PE.items() if k.upper() in common_genes}\n",
    "print(f\"Number of genes in PE after filtering: {len(PE)}\")\n",
    "master_pos_def = master_pos_def[master_pos_def[\"gene_symbol\"].isin(common_genes)]\n",
    "# Keep only first occurrence of each gene symbol to ensure uniqueness\n",
    "master_pos_def = master_pos_def.drop_duplicates(subset=['gene_symbol'], keep='first')\n",
    "\n",
    "print(f\"Number of genes in master_pos_def after filtering: {len(master_pos_def)}\")\n",
    "\n",
    "if len(PE) != len(master_pos_def[\"gene_symbol\"].unique()):\n",
    "    print(f\"Number of genes in PE: {len(PE)}\")\n",
    "    print(f\"Number of genes in master_pos_def: {len(master_pos_def['gene_symbol'].unique())}\")\n",
    "    raise ValueError(\"Number of genes in PE and master_pos_def are not the same\")\n",
    "\n",
    "FILTERED_SPECIES_PROTEIN_EMBEDDINGS_PATH = SPECIES_PROTEIN_EMBEDDINGS_PATH[:-4] + \"_filtered.pt\"\n",
    "torch.save(PE, FILTERED_SPECIES_PROTEIN_EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "541baded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/platypus_to_chrom_pos.csv\n"
     ]
    }
   ],
   "source": [
    "chrom_file = f\"{OUTPUT_DIR}/{SPECIES_NAME}_to_chrom_pos.csv\"\n",
    "master_pos_def.to_csv(chrom_file, index=False) # Save the DF\n",
    "# The chromosome file path will be:\n",
    "print(chrom_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e277f",
   "metadata": {},
   "source": [
    "# Generate token file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2798848",
   "metadata": {},
   "source": [
    "This will create the token file. Please note the offset value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4355dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10509/3083231355.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_pe = torch.load(\"/mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/all_tokens.torch\")[0:4] # read in existing token file to make sure\n",
      "/tmp/ipykernel_10509/3083231355.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  PE = torch.load(FILTERED_SPECIES_PROTEIN_EMBEDDINGS_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHROM_TOKEN_OFFSET: 11333\n",
      "Saving PE tokens to /mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/platypus_pe_tokens.torch\n",
      "Saving offsets to /mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/platypus_offsets.pkl\n",
      "Saved PE, offsets file\n"
     ]
    }
   ],
   "source": [
    "token_dim = 5120\n",
    "species_to_offsets = {}\n",
    "\n",
    "N_UNIQ_CHROM = len(master_pos_def[master_pos_def[\"species\"] == species][\"chromosome\"].unique())\n",
    "\n",
    "all_pe = torch.load(\"/mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/all_tokens.torch\")[0:4] # read in existing token file to make sure \n",
    "# that special vocab tokens are the same for different seeds\n",
    "\n",
    "offset = len(all_pe) # special tokens at the top!\n",
    "\n",
    "PE = torch.load(FILTERED_SPECIES_PROTEIN_EMBEDDINGS_PATH)\n",
    "\n",
    "pe_stacked = torch.stack(list(PE.values()))\n",
    "all_pe = torch.vstack((all_pe, pe_stacked))\n",
    "species_to_offsets[species] = offset\n",
    "\n",
    "CHROM_TOKEN_OFFSET = all_pe.shape[0]\n",
    "print(\"CHROM_TOKEN_OFFSET:\", CHROM_TOKEN_OFFSET)\n",
    "torch.manual_seed(TAXONOMY_ID)\n",
    "CHROM_TENSORS = torch.normal(mean=0, std=1, size=(N_UNIQ_CHROM, 5120)) \n",
    "# N_UNIQ_CHROM is the total number of chromosome choices, it is hardcoded for now (for species in the training data)\n",
    "all_pe = torch.vstack(\n",
    "    (all_pe, CHROM_TENSORS))  # Add the chrom tensors to the end\n",
    "all_pe.requires_grad = False\n",
    "\n",
    "assert all_pe.size(0) == master_pos_def.shape[0] + 4 + N_UNIQ_CHROM, f\"all_pe.size(0): {all_pe.size(0)}, master_pos_def.shape[0]: {master_pos_def.shape[0]}, N_UNIQ_CHROM: {N_UNIQ_CHROM}\"\n",
    "\n",
    "pe_tokens_file = f\"{OUTPUT_DIR}/{SPECIES_NAME}_pe_tokens.torch\"\n",
    "print(f\"Saving PE tokens to {pe_tokens_file}\")\n",
    "torch.save(all_pe, pe_tokens_file)\n",
    "\n",
    "offsets_file = f\"{OUTPUT_DIR}/{SPECIES_NAME}_offsets.pkl\"\n",
    "print(f\"Saving offsets to {offsets_file}\")\n",
    "with open(offsets_file, \"wb+\") as f:\n",
    "    pickle.dump(species_to_offsets, f)\n",
    "print(\"Saved PE, offsets file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aed61cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_UNIQ_CHROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "709c20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'platypus': 4}\n"
     ]
    }
   ],
   "source": [
    "with open(offsets_file, \"rb\") as f:\n",
    "    offsets = pickle.load(f)\n",
    "print(offsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c6bca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated /mnt/czi-sci-ai/generate-cross-species-secondary/eval/baselines/uce/33l_8ep_1024t_1280/model_files/new_species_protein_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing CSV or create new one if it doesn't exist\n",
    "try:\n",
    "    embeddings_df = pd.read_csv(NEW_SPECIES_CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    embeddings_df = pd.DataFrame(columns=['species', 'path', 'chrom_token_offset'])\n",
    "\n",
    "# Add new row\n",
    "new_row = pd.DataFrame({\n",
    "    'species': [SPECIES_NAME],\n",
    "    'path': [FILTERED_SPECIES_PROTEIN_EMBEDDINGS_PATH],\n",
    "    'chrom_token_offset': [CHROM_TOKEN_OFFSET]\n",
    "})\n",
    "\n",
    "# Combine and remove duplicates based on species\n",
    "embeddings_df = pd.concat([embeddings_df, new_row])\n",
    "embeddings_df = embeddings_df.drop_duplicates(subset=['species'], keep='last')\n",
    "\n",
    "# Save updated CSV\n",
    "embeddings_df.to_csv(NEW_SPECIES_CSV_PATH, index=False)\n",
    "print(f\"Updated {NEW_SPECIES_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4697330",
   "metadata": {},
   "source": [
    "# Example evaluation of new species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72667d",
   "metadata": {},
   "source": [
    "**Note: when you evaluate a new species, you need to change some arguments and modify some files:**\n",
    "\n",
    "You will  need to modify the csv in `model_files/new_species_protein_embeddings.csv` to include the new protein embeddings file you downloaded.\n",
    "\n",
    "In the file add a row for the new species with the format:\n",
    "`species name,full path to protein embedding file`\n",
    "\n",
    "Please also add this line to the dictionary created on line 247 in the file `data_proc/data_utils.py`.\n",
    "\n",
    "When you want to embed this new species, you will need to specify these newly created files as arguments.\n",
    "- `CHROM_TOKEN_OFFSET`: This tells UCE when the rows corresponding to chromosome tokens starts.\n",
    "- `spec_chrom_csv_path`: This is a new csv, created by this script, which maps genes to chromosomes and genomic positions\n",
    "- `token_file`: This is a new token file that will work just for this species. The embeddings generated will still be universal though!\n",
    "- `offset_pkl_path`: This is another file that maps genes to tokens\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "accelerate launch eval_single_anndata.py chicken_heart.h5ad --species=chicken --CHROM_TOKEN_OFFSET=13275 --spec_chrom_csv_path=data_proc/chicken_to_chrom_pos.csv --token_file=data_proc/chicken_pe_tokens.torch --offset_pkl_path=data_proc/chicken_offsets.pkl --dir=... --multi_gpu=True\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39ce8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f19bff55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
